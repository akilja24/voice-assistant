# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

# voice-assistant â€“ Modular AI Voice Backend

## Overview

This project implements a modular voice assistant backend with four services:

| Component         | Role                            | Runtime                      |
|------------------|----------------------------------|------------------------------|
| orchestrator      | Main API gateway / coordinator   | Docker (FastAPI)             |
| whisper-service   | Audio â†’ Text (ASR)               | Docker (CUDA 12.2)           |
| Ollama (external) | Text â†’ Response (LLM)            | Native host service          |
| tts-service       | Text â†’ Audio (TTS)               | Docker (CPU only)            |

All services are REST-based. The orchestrator is the single public entry point that manages all downstream interactions.

## ğŸ”— Component Repositories

- **faster-whisper (ASR)**  
  https://github.com/guillaumekln/faster-whisper

- **Piper TTS (local speech)**  
  https://github.com/rhasspy/piper

- **Ollama (LLM runtime)**  
  https://github.com/ollama/ollama  
  Docs: https://ollama.com

## ğŸ” Architecture and Data Flow

```
[Client]
   â”‚
   â–¼
POST /interact (audio) â”€â”€â”€â–º orchestrator
                             â”œâ”€â”€â–º whisper-service (transcribe)
                             â”œâ”€â”€â–º Ollama (generate)
                             â””â”€â”€â–º tts-service (speak)
                                           â”‚
                                       return audio
```

### Interaction Flow

1. Client uploads an audio file to `POST /interact` on `orchestrator`.
2. Orchestrator:
   - Sends audio to `whisper-service`
   - Sends transcription to `Ollama` (running at `http://host.docker.internal:11434`)
   - Sends generated text to `tts-service`
   - Returns synthesized audio to the client

## ğŸ“‚ Project Structure

```
voice-assistant/
â”œâ”€â”€ compose.yaml
â”œâ”€â”€ CLAUDE.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/ci.yml
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ app/main.py
â”œâ”€â”€ whisper-service/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ app/main.py
â”œâ”€â”€ tts-service/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ app/main.py
```

## âš™ï¸ Environment Variables

| Variable         | Used by         | Description                              |
|------------------|------------------|------------------------------------------|
| WHISPER_MODEL     | whisper-service  | e.g. `tiny.en`                            |
| TTS_VOICE         | tts-service      | e.g. `en_US-amy-medium`                   |
| SHARED_SECRET     | all              | Optional API key for internal services   |
| OLLAMA_MODEL      | orchestrator     | e.g. `llama3`, `mistral`, `phi3`, etc.    |
| OLLAMA_URL        | orchestrator     | e.g. `http://host.docker.internal:11434` |

## âš™ï¸ Docker Compose Notes

- Each service is containerized except Ollama, which must be installed natively.


## ğŸ§ª Local Dev & Testing

```bash
docker compose build
docker compose up -d
```

### Health Checks

```bash
curl http://localhost:8000/health       # orchestrator
curl http://localhost:8001/health       # whisper
curl http://localhost:8003/health       # tts
```


## ğŸ§¼ Code Quality & Tooling

- **Format:** Black, isort
- **Commits:** Conventional Commits (`feat:`, `fix:`, `chore:`, etc.)
- **CI/CD:** GitHub Actions via `.github/workflows/ci.yml`
- **Pre-commit hooks:** see `.pre-commit-config.yaml`

## âœ… Future Tasks

- [ ] Add streaming mode via WebSockets to orchestrator
- [ ] Optional fallback to OpenAI for LLM or TTS if local models fail
- [ ] Add CLI or minimal front-end interface